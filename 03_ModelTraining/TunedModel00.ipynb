{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92b5aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries.\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Libraries for extracting and labelling data.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "#Libraries for implementation of CNN.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "\n",
    "#Libraries for training the model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math#For rounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "536b250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable parameters\n",
    "v_dataset_size = 45000 #Number of data used for tuning the model\n",
    "VALIDATION_DATASET_SIZE = 5000\n",
    "\n",
    "#\n",
    "COMBINATION_COUNT_FOR_EACH_MFCC = 30\n",
    "\n",
    "#seed to reproduce output\n",
    "RANDOM_SEED_VALUE = 10\n",
    "\n",
    "#2 possible outputs, chorus and vanilla\n",
    "NUM_OF_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05659a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Parameters\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "#Folder paths\n",
    "DATA_FOLDER = 'data232\\\\'\n",
    "METADATA_FOLDER = 'metadata232\\\\'\n",
    "METADATA_FILENAME = 'metadata232.csv'\n",
    "TESTDATA_FOLDER = 'testdata230\\\\'\n",
    "MODEL_FOLDER = 'tunedmodel00\\\\'\n",
    "SYNTH_DATA_FOLDER = 'testdatasynth00\\\\'\n",
    "\n",
    "CSV_READ_CHUNK_SIZE = 2000\n",
    "\n",
    "#Maximum length of an input soundclip supported. Any audio longer than ..\n",
    "#..this is not considered.\n",
    "SAMPLE_LENGTH_SEC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d846ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for each parameter a list of 4 values are given.\n",
    "#Each index of the list corrsponds to the each of the 4 combinations obtained ..\n",
    "#..as having higher accuracy values.\n",
    "#E.g.: If we want to use the 2nd set of values, we need to pick the 2nd value ..\n",
    "#..from each list. E.g.: v_n_mfcc_vals[1], h_layers_count_vals[1],....etc.\n",
    "\n",
    "#Variable parameters - PROD\n",
    "v_n_mfcc_vals = [128,128,128,128]\n",
    "\n",
    "#Variable hyperparameters\n",
    "h_epochs_vals = [5,5,5,10]\n",
    "h_batch_size_vals = [32,64,16,16]\n",
    "\n",
    "#CNN hyperparameters\n",
    "#Convolutional layers\n",
    "h_layers_count_vals = [4,4,5,4]\n",
    "h_filter_size_vals = [32,32,32,32]\n",
    "h_kernel_size_vals = [(3,3),(3,3),(3,3),(3,3)]\n",
    "h_strides_vals = [(1,1),(1,1),(1,1),(1,1)]\n",
    "h_activation_function_vals = ['relu','relu','relu','relu']\n",
    "h_max_pooling_pool_size_vals = [(2,2),(2,2),(2,2),(2,2)]\n",
    "h_max_pooling_strides_vals = [(2,2),(2,2),(2,2),(2,2)]\n",
    "h_dropout_rate_vals = [0.6,0.5,0.2,0.3]\n",
    "\n",
    "#Flattening layer\n",
    "h_flatten_dropout_rate_vals = [0.4,0.4,0.4,0.6]\n",
    "\n",
    "#Output function\n",
    "h_output_activation_function_vals = ['softmax','softmax','softmax','softmax']\n",
    "\n",
    "#Model training parameters\n",
    "h_loss_function_vals = ['binary_crossentropy','binary_crossentropy','categorical_crossentropy','binary_crossentropy']\n",
    "h_optimizer_vals = ['adam','adam','adam','adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdb29186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from each audio file.\n",
    "def features_extractor(audio_file_path,n_mfcc):\n",
    "    #Selection of res_type => https://librosa.org/doc/main/generated/librosa.resample.html#librosa.resample\n",
    "    #Faster method is selected\n",
    "    audio, sample_rate = librosa.load(audio_file_path, res_type='kaiser_fast',sr=SAMPLE_RATE)\n",
    "    \n",
    "    if(len(audio)>sample_rate*SAMPLE_LENGTH_SEC):\n",
    "        audio = audio[:sample_rate*SAMPLE_LENGTH_SEC]\n",
    "    \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return mfccs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a15abe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to automate testing the trained model for each synthesized sound ..\n",
    "#..and calculating the overall accuracy.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_synth_accuracy(data_folder,model,n_mfcc,input_width):\n",
    "    X_synthtest = []\n",
    "    y_synthtest = []\n",
    "    \n",
    "    #Picking only .wav files\n",
    "    wav_file_paths = glob.glob(data_folder+'*.wav')\n",
    "    \n",
    "    for wav_file_path in wav_file_paths:\n",
    "        wav_file_features1 = features_extractor(wav_file_path,n_mfcc)    \n",
    "        wav_file_features = pad_features(wav_file_features1,n_mfcc,input_width)\n",
    "    \n",
    "        X_synthtest.append(wav_file_features)\n",
    "    \n",
    "        wav_file_name = os.path.basename(wav_file_path)\n",
    "        split_file_name = wav_file_name.split('_')\n",
    "\n",
    "        #For easier identification;\n",
    "        #-a non-chorus file name is in the format 'v_*.wav'\n",
    "        #-a chorus file name is in the format 'c_*.wav'\n",
    "        #The naming convention is used to label the files here.\n",
    "        if(split_file_name[0]=='v'):\n",
    "            wav_file_class = 0\n",
    "        elif(split_file_name[0]=='c'):\n",
    "            wav_file_class = 1\n",
    "        else:\n",
    "            wav_file_class = -1\n",
    "\n",
    "        #Printing prediction for each file\n",
    "        label = model.predict(np.array([wav_file_features]))\n",
    "        classes_x=np.argmax(label,axis=1)\n",
    "        prediction_class = labelencoder.inverse_transform(classes_x)\n",
    "        print(str(wav_file_name)+' || label = '+str(label)+' || prediction = '+str(prediction_class))\n",
    "\n",
    "        y_synthtest.append(wav_file_class)\n",
    "        \n",
    "    len_Xsynthtest = len(X_synthtest)\n",
    "    len_ysynthtest = len(y_synthtest)\n",
    "\n",
    "    X_synthtest=np.array(X_synthtest).reshape(len_Xsynthtest,n_mfcc,input_width,1)\n",
    "    y_synthtest=to_categorical(labelencoder.fit_transform(y_synthtest))\n",
    "    y_synthtest=y_synthtest.reshape(len_ysynthtest,2)\n",
    "\n",
    "    synth_accuracy=model.evaluate(X_synthtest,y_synthtest,verbose=0)\n",
    "\n",
    "    return synth_accuracy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f29c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea for cleanly writing padding function is taken from the below 2 posts.\n",
    "#https://stackoverflow.com/questions/59241216/padding-numpy-arrays-to-a-specific-size\n",
    "#https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab\n",
    "def pad_features(feature_array,expected_height,expected_width):\n",
    "    array_height = feature_array.shape[0]\n",
    "    array_width = feature_array.shape[1]\n",
    "    \n",
    "    pad_height = max(expected_height-array_height,0)\n",
    "    pad_height1 = math.floor(pad_height/2)\n",
    "    pad_height2 = max(pad_height-pad_height1,0)\n",
    "    \n",
    "    pad_width = max(expected_width-array_width,0)\n",
    "    pad_width1 = math.floor(pad_width/2)\n",
    "    pad_width2 = max(pad_width-pad_width1,0)\n",
    "    \n",
    "    #print('h->'+str(pad_height)+'|| w->'+str(pad_width))\n",
    "    return np.pad(array=feature_array,pad_width=((pad_height1,pad_height2),\n",
    "                                                 (pad_width1,pad_width2)),mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "065273d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read stored metadata and get a Pandas dataframe\n",
    "file_path = METADATA_FOLDER+METADATA_FILENAME\n",
    "\n",
    "used_columns = ['filename','chorus']\n",
    "\n",
    "dtypes = {\n",
    "    'filename': 'str',\n",
    "    'chorus': 'int'\n",
    "}\n",
    "\n",
    "data_chunks = pd.read_csv(file_path, usecols=used_columns,\n",
    "                          dtype=dtypes, chunksize=CSV_READ_CHUNK_SIZE)\n",
    "\n",
    "# concatenate the chunks into a single DataFrame\n",
    "df = pd.concat(data_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c4d2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 128, 173)\n",
      "(45000, 2)\n",
      "h_max_pooling_pool_size =>(2, 2)\n",
      "layer_no =>1\n",
      "h_max_pooling_strides =>(2, 2)\n",
      "h_max_pooling_pool_size =>(2, 2)\n",
      "layer_no =>2\n",
      "h_max_pooling_strides =>(2, 2)\n",
      "h_max_pooling_pool_size =>(2, 2)\n",
      "layer_no =>3\n",
      "h_max_pooling_strides =>(2, 2)\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9657\n",
      "Epoch 1: val_loss improved from inf to 0.01323, saving model to tunedmodel00\\tuned_model_00.hdf5\n",
      "500/500 [==============================] - 1581s 3s/step - loss: 0.2070 - accuracy: 0.9657 - val_loss: 0.0132 - val_accuracy: 0.9985\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.6265e-04 - accuracy: 0.9999\n",
      "Epoch 2: val_loss improved from 0.01323 to 0.00542, saving model to tunedmodel00\\tuned_model_00.hdf5\n",
      "500/500 [==============================] - 1516s 3s/step - loss: 2.6265e-04 - accuracy: 0.9999 - val_loss: 0.0054 - val_accuracy: 0.9987\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.5465e-04 - accuracy: 0.9999\n",
      "Epoch 3: val_loss improved from 0.00542 to 0.00072, saving model to tunedmodel00\\tuned_model_00.hdf5\n",
      "500/500 [==============================] - 3533s 7s/step - loss: 1.5465e-04 - accuracy: 0.9999 - val_loss: 7.2281e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - ETA: 0s - loss: 7.6130e-05 - accuracy: 1.0000\n",
      "Epoch 4: val_loss improved from 0.00072 to 0.00027, saving model to tunedmodel00\\tuned_model_00.hdf5\n",
      "500/500 [==============================] - 1597s 3s/step - loss: 7.6130e-05 - accuracy: 1.0000 - val_loss: 2.6940e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - ETA: 0s - loss: 3.8830e-06 - accuracy: 1.0000\n",
      "Epoch 5: val_loss did not improve from 0.00027\n",
      "500/500 [==============================] - 3054s 6s/step - loss: 3.8830e-06 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 0.9999\n",
      "Training completed in time:  3:08:02.325243\n"
     ]
    }
   ],
   "source": [
    "#Initialization of extracted data\n",
    "extracted_X = []\n",
    "extracted_y = []        \n",
    "\n",
    "#Read metadata on the dataset to fetch .wav file names.\n",
    "for index, row in df.iterrows():\n",
    "#Limit fetching the data when it reaches the specified dataset size\n",
    "    if (index>v_dataset_size-1):\n",
    "        break\n",
    "\n",
    "    file_name = row['filename']\n",
    "    class_label = row['chorus']\n",
    "\n",
    "    features = features_extractor(DATA_FOLDER+file_name,v_n_mfcc_vals[1])    \n",
    "    extracted_X.append(features)\n",
    "    extracted_y.append(class_label)\n",
    "\n",
    "#Convert to numpy arrays\n",
    "extracted_X = np.array(extracted_X)\n",
    "extracted_y = np.array(extracted_y)    \n",
    "\n",
    "#Output classes\n",
    "labelencoder=LabelEncoder()\n",
    "extracted_y=to_categorical(labelencoder.fit_transform(extracted_y))\n",
    "\n",
    "print(extracted_X.shape)\n",
    "print(extracted_y.shape)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(extracted_X[:v_dataset_size-VALIDATION_DATASET_SIZE],\n",
    "                                               extracted_y[:v_dataset_size-VALIDATION_DATASET_SIZE],\n",
    "                                               test_size=0.2,random_state=1)\n",
    "\n",
    "#Setting seed values to get reproducible outputs and same random values for each hyperparameter set.\n",
    "np.random.seed(RANDOM_SEED_VALUE)\n",
    "tf.random.set_seed(RANDOM_SEED_VALUE)    \n",
    "python_random.seed(RANDOM_SEED_VALUE)\n",
    "\n",
    "#input shape of extracted mfcc 2D matrix\n",
    "input_shape = (v_n_mfcc_vals[1],extracted_X.shape[2],1)\n",
    "\n",
    "#Assignment of hyperparameters.\n",
    "h_layers_count = h_layers_count_vals[1]\n",
    "h_filter_size = h_filter_size_vals[1]\n",
    "h_kernel_size = h_kernel_size_vals[1]\n",
    "h_strides = h_strides_vals[1]\n",
    "h_activation_function = h_activation_function_vals[1]\n",
    "h_max_pooling_pool_size = h_max_pooling_pool_size_vals[1]\n",
    "h_max_pooling_strides = h_max_pooling_strides_vals[1]\n",
    "h_dropout_rate = h_dropout_rate_vals[1]\n",
    "h_flatten_dropout_rate = h_flatten_dropout_rate_vals[1]\n",
    "h_output_activation_function = h_output_activation_function_vals[1]\n",
    "h_loss_function = h_loss_function_vals[1]\n",
    "h_optimizer = h_optimizer_vals[1]\n",
    "h_epochs = h_epochs_vals[1]\n",
    "h_batch_size = h_batch_size_vals[1]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "\n",
    "#CNN Layer 1 with mirrored padding after the input layer\n",
    "conv_layer1 = Conv2D(filters=h_filter_size,\n",
    "                     kernel_size=h_kernel_size,\n",
    "                     strides=h_strides,\n",
    "                     padding='same',\n",
    "                     data_format='channels_last',\n",
    "                     activation=h_activation_function,\n",
    "                     input_shape=input_shape)\n",
    "maxpool_layer1 = MaxPooling2D(pool_size=h_max_pooling_pool_size, strides=h_max_pooling_strides, padding='valid')\n",
    "dropout_layer1 = Dropout(rate=h_dropout_rate)\n",
    "\n",
    "model.add(conv_layer1)\n",
    "\n",
    "\n",
    "#Add convolutional layers\n",
    "for layer_no in range(1,h_layers_count):\n",
    "    conv_layer_i = Conv2D(filters=h_filter_size*(layer_no+1),\n",
    "                          kernel_size=h_kernel_size,\n",
    "                          strides=h_strides,\n",
    "                          padding='same',\n",
    "                          data_format='channels_last',\n",
    "                          activation=h_activation_function)\n",
    "    print('h_max_pooling_pool_size =>'+str(h_max_pooling_pool_size))\n",
    "    print('layer_no =>'+str(layer_no))\n",
    "    print('h_max_pooling_strides =>'+str(h_max_pooling_strides))\n",
    "\n",
    "    maxpool_layer_i = MaxPooling2D(pool_size=h_max_pooling_pool_size, strides=h_max_pooling_strides, padding='valid')\n",
    "    dropout_layer_i = Dropout(rate=h_dropout_rate)\n",
    "\n",
    "    model.add(conv_layer_i)\n",
    "    model.add(maxpool_layer_i)\n",
    "    model.add(dropout_layer_i)\n",
    "\n",
    "# Add a flattening layer after the dropout layer\n",
    "flatten_layer = Flatten()\n",
    "dropout_layer = Dropout(rate=h_flatten_dropout_rate)\n",
    "\n",
    "#Output Layer\n",
    "output_layer=Dense(units=NUM_OF_CLASSES,activation=h_output_activation_function)\n",
    "\n",
    "model.add(flatten_layer)\n",
    "model.add(dropout_layer)\n",
    "model.add(output_layer)\n",
    "\n",
    "#Compiling the model\n",
    "model.compile(loss=h_loss_function,metrics=['accuracy'],optimizer=h_optimizer)\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_FOLDER+'tuned_model_00'+'.hdf5',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "#Training the model\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=h_batch_size,\n",
    "          epochs=h_epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[checkpointer],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "m_training_duration = datetime.now() - start\n",
    "print('Training completed in time: ', m_training_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "890e74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "c_1.wav || label = [[0.9565343  0.04346563]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "c_10.wav || label = [[3.1485415e-13 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "c_11.wav || label = [[1.3725155e-14 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "c_12.wav || label = [[5.5170164e-04 9.9944824e-01]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "c_13.wav || label = [[0.00503445 0.9949655 ]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "c_14.wav || label = [[3.351409e-08 1.000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "c_15.wav || label = [[6.306799e-09 1.000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "c_2.wav || label = [[9.9998689e-01 1.3061623e-05]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "c_3.wav || label = [[0.0080953 0.9919046]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "c_4.wav || label = [[1.6758225e-11 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "c_5.wav || label = [[2.3912354e-09 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "c_6.wav || label = [[2.373655e-12 1.000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "c_7.wav || label = [[3.4896792e-14 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "c_8.wav || label = [[9.9909854e-01 9.0148282e-04]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "c_9.wav || label = [[6.2228894e-10 1.0000000e+00]] || prediction = [1]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "v_1.wav || label = [[1.0000000e+00 3.2227766e-13]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "v_10.wav || label = [[1.0000000e+00 1.7514258e-08]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "v_11.wav || label = [[1.0000000e+00 3.9472904e-12]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "v_12.wav || label = [[1.000000e+00 9.748877e-13]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "v_13.wav || label = [[1.0000000e+00 7.0566024e-16]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "v_14.wav || label = [[1.0000000e+00 2.3906753e-12]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "v_15.wav || label = [[9.9990904e-01 9.0892921e-05]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "v_2.wav || label = [[1.000000e+00 1.838523e-16]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "v_3.wav || label = [[1.000000e+00 5.681095e-16]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "v_4.wav || label = [[1.0000000e+00 2.2216724e-14]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "v_5.wav || label = [[1.0000000e+00 8.6489365e-15]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "v_6.wav || label = [[1.0000000e+00 6.3227185e-10]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "v_7.wav || label = [[1.00000000e+00 1.20032935e-08]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "v_8.wav || label = [[1.0000000e+00 9.5822223e-17]] || prediction = [0]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "v_9.wav || label = [[1.0000000e+00 6.2471944e-11]] || prediction = [0]\n"
     ]
    }
   ],
   "source": [
    " #Measuring the performance of the trained model\n",
    "\n",
    "#Validation accuracy\n",
    "validation_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "m_validation_accuracy = validation_accuracy[1]\n",
    "\n",
    "#Test accuracy\n",
    "test_accuracy = model.evaluate(extracted_X[v_dataset_size-VALIDATION_DATASET_SIZE:],\n",
    "                              extracted_y[v_dataset_size-VALIDATION_DATASET_SIZE:],\n",
    "                              verbose=0)\n",
    "m_test_accuracy = test_accuracy[1]\n",
    "\n",
    "m_synth_accuracy = get_synth_accuracy(SYNTH_DATA_FOLDER,model,v_n_mfcc_vals[1],extracted_X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9267dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance----\n",
      "Validation data accuracy = 0.999875009059906\n",
      "Test data accuracy = 0.9995999932289124\n",
      "Synth data accuracy = 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "print('Model Performance----')\n",
    "print('Validation data accuracy = '+str(m_validation_accuracy))\n",
    "print('Test data accuracy = '+str(m_test_accuracy))\n",
    "print('Synth data accuracy = '+str(m_synth_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "574f168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 128, 173, 32)      320       \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 128, 173, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 64, 86, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64, 86, 64)        0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 64, 86, 96)        55392     \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 32, 43, 96)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 32, 43, 96)        0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 32, 43, 128)       110720    \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 16, 21, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 16, 21, 128)       0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 43008)             0         \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 43008)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 86018     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 270,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
